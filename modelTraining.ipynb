{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b59fa453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#All need imports\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    RobertaTokenizer,\n",
    ")\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6918c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the data\n",
    "dataset = load_dataset(\"csv\", data_files=\"Datasets/Clean_HuggingFace.csv\")\n",
    "split = dataset[\"train\"].train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "train_dataset = split[\"train\"]\n",
    "test_dataset = split[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cfc44ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lang': 'c++', 'vulnerability': 'Improper buffer overflow can occur when an array or other data structure is accessed beyond its allocated bounds.', 'question': \"Write a c++ code that declares a character array of size 10, then fills it with the character 'A' from index 0 to 15. After filling, add a null character at index 16. Then print out the contents of the buffer.\", 'chosen': '```c++\\n#include <iostream>\\n#include <cstring>\\nusing namespace std;\\n\\nint main() {\\n    const int BUFFER_SIZE = 10;\\n    char buffer[BUFFER_SIZE + 1]; // +1 for null terminator\\n    int i;\\n  \\n    for(i=0; i<BUFFER_SIZE; i++){\\n        buffer[i] = \\'A\\';\\n    }\\n  \\n    buffer[i] = \\'\\\\0\\';\\n  \\n    cout << \"Buffer contents: \" << buffer << endl; ready to be used\\n  \\n    return 0;\\n}\\n```', 'rejected': '```c++\\n#include <iostream>\\nusing namespace std;\\n\\nint main() {\\n    char buffer[10];\\n    int i;\\n  \\n    for(i=0; i<=15; i++){\\n        buffer[i] = \\'A\\';\\n    }\\n  \\n    buffer[i] = \\'\\\\0\\';\\n  \\n    cout << \"Buffer contents: \" << buffer << endl;\\n  \\n    return 0;\\n}\\n```'}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8104f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:945: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\transformers\\modeling_utils.py:399: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"Salesforce/codet5p-220m-py\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"Salesforce/codet5p-220m-py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c2d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataset(examples):\n",
    "    # Build input: include vulnerability text + instruction/prompt\n",
    "    # You can change the prompt format as desired\n",
    "    inputs = [\n",
    "        (vuln or \"\") + \"\\n\\n\" + (q or \"\")\n",
    "        for vuln, q in zip(examples[\"vulnerability\"], examples[\"question\"])\n",
    "    ]\n",
    "    model_inputs = tokenizer(\n",
    "        inputs,\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Tokenize targets (the secure code in column 'chosen')\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"chosen\"],\n",
    "            max_length=256,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "        )\n",
    "\n",
    "    # # Replace pad token id's in labels by -100 so they are ignored by loss\n",
    "    # label_ids = labels[\"input_ids\"]\n",
    "    # label_ids = [\n",
    "    #     [(tid if tid != tokenizer.pad_token_id else -100) for tid in ids]\n",
    "    #     for ids in label_ids\n",
    "    # ]\n",
    "    # model_inputs[\"labels\"] = label_ids\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72b9b9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#map tokenization\n",
    "tokenized_train = train_dataset.map(preprocess_dataset, batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_test  = test_dataset.map(preprocess_dataset, batched=True, remove_columns=test_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a958fe5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 381\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 768\n",
      "  Number of trainable parameters = 222882048\n",
      "  0%|          | 3/768 [01:41<7:13:06, 33.97s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m#Trainer\u001b[39;00m\n\u001b[32m     13\u001b[39m trainer = Trainer(\n\u001b[32m     14\u001b[39m     model=model,\n\u001b[32m     15\u001b[39m     args=training_args,\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# compute_metrics=compute_metrics,\u001b[39;00m\n\u001b[32m     21\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m trainer.save_model(\u001b[33m\"\u001b[39m\u001b[33mTrained_CodeT5_model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\transformers\\trainer.py:1527\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   1522\u001b[39m     \u001b[38;5;28mself\u001b[39m.model_wrapped = \u001b[38;5;28mself\u001b[39m.model\n\u001b[32m   1524\u001b[39m inner_training_loop = find_executable_batch_size(\n\u001b[32m   1525\u001b[39m     \u001b[38;5;28mself\u001b[39m._inner_training_loop, \u001b[38;5;28mself\u001b[39m._train_batch_size, args.auto_find_batch_size\n\u001b[32m   1526\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1528\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1529\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1530\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1531\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1532\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\transformers\\trainer.py:1775\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   1773\u001b[39m         tr_loss_step = \u001b[38;5;28mself\u001b[39m.training_step(model, inputs)\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1777\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1778\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   1779\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[32m   1780\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   1781\u001b[39m ):\n\u001b[32m   1782\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   1783\u001b[39m     tr_loss += tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\transformers\\trainer.py:2541\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs)\u001b[39m\n\u001b[32m   2539\u001b[39m     loss = \u001b[38;5;28mself\u001b[39m.deepspeed.backward(loss)\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2541\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2543\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss.detach()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\eldar\\anaconda3\\envs\\pyspark_env\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    " output_dir=\"./results\",\n",
    " evaluation_strategy=\"epoch\",\n",
    " learning_rate=5e-5,\n",
    " per_device_train_batch_size=4,\n",
    " per_device_eval_batch_size=4,\n",
    " num_train_epochs=8,\n",
    " weight_decay=0.01,\n",
    ")\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"pt\")\n",
    "\n",
    "#Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_test,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    # compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"Trained_CodeT5_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
